{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ8jh5aiy8zo"
   },
   "source": [
    "# HOME ASSIGNMENT #6: CLOUD FUNCTION & STREAMLIT\n",
    "\n",
    "**Mục đích của bài Assignment**\n",
    "> *  [Optional] Data Deploy Cloud Function \n",
    "> *  Tạo Data Apps với Streamlit\n",
    "> *  Thao tác với data bằng Pandas\n",
    "> *  Data Visualization\n",
    "\n",
    "**Các kiến thức áp dụng**\n",
    "* Slack API, JSON to DataFrame\n",
    "* GCP Cloud Function\n",
    "* Streamlit\n",
    "* Python Pandas\n",
    "* Python Data Visualization\n",
    "\n",
    "**Lời Khuyên**\n",
    "* Các bạn dành thời gian ôn lại và xâu chuỗi kiến thức\n",
    "* Review Assignment 1-5 cho ít nhất 2 bạn học viên khác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE            \u001b[34massignment_3\u001b[m\u001b[m       \u001b[34massignment_6\u001b[m\u001b[m       \u001b[34mimg\u001b[m\u001b[m\r\n",
      "README.md          \u001b[34massignment_4\u001b[m\u001b[m       \u001b[34mdata\u001b[m\u001b[m               \u001b[34mpython-for-data\u001b[m\u001b[m\r\n",
      "\u001b[34massignment_2\u001b[m\u001b[m       \u001b[34massignment_5\u001b[m\u001b[m       \u001b[34mgcp-cloud-function\u001b[m\u001b[m \u001b[34mstreamlit\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 1: Python Data Viz\n",
    "Hoàn tất các sets bài tập trên [Kaggle Data Visualization](https://www.kaggle.com/learn/data-visualization) - Nếu chưa hoàn thành trong [Assignment 5](https://github.com/anhdanggit/atom-assignments/blob/main/assignment_5/home_assignment_5.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Completed in Assignment 5: https://github.com/saturn1101/atom-assignments/tree/phuong-assignments/assignment_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t28PUQoNzy1k"
   },
   "source": [
    "# TODO 2 (OPTIONAL): DEPLOY GOOGLE CLOUD FUNCTION\n",
    "* Làm theo Lab của Week 6: [HERE](https://anhdang.gitbook.io/datacracy/atom/6-cloud-function-and-streamlit/6.2-lab-cloud-function-hands-on)\n",
    "* Click đôi vào các tab Markdown bên dưới để trả lời các câu hỏi ([Markdown Cheatsheet](https://guides.github.com/features/mastering-markdown/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screenshot Cloud Function on GCP\n",
    "> *Copy Screenshot vào folder img trong repo, và đổi link bên dưới*\n",
    "\n",
    "![Cloud Function Deployed](../img/deploy_cloud_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screenshot Cloud Function Testing on GCP\n",
    "> *Copy Screenshot vào folder img trong repo, và đổi link bên dưới*\n",
    "\n",
    "![Cloud Function Testing](../img/test_cloud_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QUVZlLm00PG"
   },
   "source": [
    "## Screenshot Cloud Function Call on Postman\n",
    "> *Copy Screenshot vào folder img trong repo, và đổi link bên dưới*\n",
    "\n",
    "![\bCloud Function on Postman](../img/postman_cloud_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5c_Lx9MyzSF"
   },
   "source": [
    "## Các lỗi gặp trong quá trình thực hiện \n",
    "*Liên kê bên dưới các lỗi bạn gặp và các giải quyết*\n",
    "\n",
    "1. **ERROR: <class 'smtplib.SMTPAuthenticationError'>** ==> Mình quên mất là mình đã remove app password nên vẫn xài password cũ nên bị báo lỗi. Sau đó mình generate lại app password mới và thay vào biến môi trường rồi deploy lại"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT_ziqVJ1COI"
   },
   "source": [
    "# TODO 3: HIỂU & DIAGRAM CODE STREAMLIT \n",
    "Mình thường khuyên các bạn mới học code rằng:\n",
    "\n",
    "> Hãy code với một cây bút chì và tờ giấy\n",
    "\n",
    "Như thế nào?\n",
    "\n",
    "1. Đầu tiên, là hình dung trong đầu: Bạn sẽ bắt đầu từ gì (`inputs`) và cho ra những gì (`output`)\n",
    "2. Rồi, để đi từ inputs đến outputs thì bạn cần thực hiện những bước nào (các `functions`)\n",
    "\n",
    "Bạn có thể vẽ ra một diagram như vậy giúp bạn:\n",
    "* Nhìn bức tranh lớn, và không bị sa đà vào tiểu tiết, syntax\n",
    "* Rõ ràng hơn về flow \n",
    "* Giúp bạn tối ưu flow trước, rồi sau đó code sẽ thuận lợi hơn\n",
    "* Rất hiệu quả để bạn debugs trong tương lai\n",
    "\n",
    "Tham khảo Diagram sau của [streamlit/data_glimpse.py](https://github.com/anhdanggit/atom-assignments/blob/main/streamlit/data_glimpse.py) và vẽ diagram theo cách hiểu của bạn cho [streamlit/datacracy_slack.py](https://github.com/anhdanggit/atom-assignments/blob/main/streamlit/datacracy_slack.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagram Data Glimpse Apps\n",
    "> Bên dưới là ví dụ Diagram của app [streamlit/data_glimpse.py](https://github.com/anhdanggit/atom-assignments/blob/main/streamlit/data_glimpse.py)\n",
    "\n",
    "![data-glimpse-diagream](../img/streamlit-Data-Glimpse-Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataCracy Slack \n",
    "> Là apps để tổng hợp lịch sử nộp bài, review và discussion của Datacracy Learners\n",
    "![Datacracy-slack-streamlit](../img/dataCracy-slack-streamlit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagram DataCracy Slack Apps\n",
    "* Xem code của app [streamlit/datacracy_slack.py](https://github.com/anhdanggit/atom-assignments/blob/main/streamlit/datacracy_slack.py)\n",
    "> *Copy Diagram bạn vẽ vào folder img trong repo, và đổi link bên dưới*\n",
    "\n",
    "![datacracy-slack-diagram](../img/datacracy_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giải thích \n",
    "Xem code của app [streamlit/datacracy_slack.py](https://github.com/anhdanggit/atom-assignments/blob/main/streamlit/datacracy_slack.py):\n",
    "\n",
    "1. Trong mỗi function (steps) trong Diagram của bạn, giải thích function làm những việc gì?\n",
    "2. Liệt kê các logics được áp dụng để xử lý data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR ANSWER\n",
    "\n",
    "## 1.\n",
    "\n",
    "## load_users_df(), load_channel_df(), load_msg_df(): \n",
    "# Lấy data từ Slack về, viết data cần thiết vào 1 dictionary sau đó chuyển thành dataframe\n",
    "\n",
    "## process_msg_data():\n",
    "# Xử lý các dataframes có sẵn để trả về msg_df hoàn thiện hơn: Extract những users đã reply trong 1 post, \n",
    "# merge các dataframes và đổi lại tên cho phù hợp, format lại các cột ngày tháng, thời gian, thêm cột tính số chữ\n",
    "\n",
    "## 2. \n",
    "# 1 - Thay đổi tên các cột cho phù hợp cho phù hợp với hoàn cảnh của app\n",
    "# 2 - Thống nhất lại format của các datetime data cho dễ nhìn và thân thiện với người dùng\n",
    "# 3 - Valid user ids là những user_id nằm trong cột user_id của dataframe user_df\n",
    "# 4 - Reviews hợp lệ là reviews nằm trong channel có tên chứa 'assignment' và Datacracy_role có chứa 'Learner'\n",
    "# loại ra những reply của user_id được cho trong input\n",
    "# 5 - Ở phần Sidebar, Đã Nộp được tính bằng chiều dài (số rows) của submit_df, viết dưới format số thập phân 2 chữ số\n",
    "# 6 - Ở Sidebar, số review được nhận được tính theo %, là số rows của submit_df thoả mãn số replies từ người khác lớn hơn 0 chia cho tổng số rows của submit_df\n",
    "# 7 - Ở Sidebar, số review đã viết chính là số rows của review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 4: VISUALIZATION ON STREAMLIT\n",
    "Áp dụng kiến thức đã học trong *TODO 1* + Pandas thực hiện các tasks sau:\n",
    "\n",
    "1. Tổng hợp cho tất cả learners các chỉ số sau: \n",
    "    * Số assignment đã nộp\n",
    "    * % bài được review \n",
    "    * Số workcount đã thảo luận \n",
    "    * Extract thứ trong tuần (weekday) của ngày nộp bài \n",
    "    * Extract giờ trong ngày nộp bài (hour)\n",
    "    \n",
    "4. Vẽ biểu đồ thể hiện phân phối (Distribution - [Kaggle Tutorial](https://www.kaggle.com/alexisbcook/distributions)) của các thông số trên và add vào app Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalHashError",
     "evalue": "module '__main__' has no attribute '__file__'\n\nWhile caching the body of `load_users_df()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function load_users_df at 0x106c72820>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/en/stable/caching.html#the-hash-funcs-parameter)\nfor more details.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# Hash the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"%s:%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_should_be_hashed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_file_should_be_hashed\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    386\u001b[0m         return file_util.file_is_in_folder_glob(\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_main_script_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         ) or file_util.file_in_pythonpath(filepath)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_get_main_script_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;31m# script path in ScriptRunner.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mmain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '__main__' has no attribute '__file__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalHashError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9c8189317930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# Table data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0muser_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_users_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mchannel_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_channel_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/caching.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshow_spinner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspinner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mget_or_create_cached_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_or_create_cached_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/caching.py\u001b[0m in \u001b[0;36mget_or_create_cached_value\u001b[0;34m()\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;31m# If we generated the key earlier we would only hash those\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0;31m# globals by name, and miss changes in their code or value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                 \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_hash_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_funcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;31m# First, get the cache that's attached to this function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/caching.py\u001b[0m in \u001b[0;36m_hash_func\u001b[0;34m(func, hash_funcs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;31m# because this step will be hashing any objects referenced in the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0;31m# body.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     update_hash(\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0mhasher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc_hasher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36mupdate_hash\u001b[0;34m(val, hasher, hash_reason, hash_source, context, hash_funcs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CodeHasher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash_funcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, hasher, obj, context)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;34m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0mhasher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInternalHashError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# Hash the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"%s:%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;31m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"md5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_should_be_hashed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__defaults__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_file_should_be_hashed\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         return file_util.file_is_in_folder_glob(\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_main_script_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         ) or file_util.file_in_pythonpath(filepath)\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/streamlit/hashing.py\u001b[0m in \u001b[0;36m_get_main_script_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# This works because we set __main__.__file__ to the report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;31m# script path in ScriptRunner.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mmain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalHashError\u001b[0m: module '__main__' has no attribute '__file__'\n\nWhile caching the body of `load_users_df()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function load_users_df at 0x106c72820>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/en/stable/caching.html#the-hash-funcs-parameter)\nfor more details.\n            "
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "st.title('DataCracy ATOM Tiến Độ Lớp Học')\n",
    "with open('./env_variable.json','r') as j:\n",
    "    json_data = json.load(j)\n",
    "\n",
    "#SLACK_BEARER_TOKEN = os.environ.get('SLACK_BEARER_TOKEN') ## Get in setting of Streamlit Share\n",
    "SLACK_BEARER_TOKEN = json_data['SLACK_BEARER_TOKEN']\n",
    "DTC_GROUPS_URL = ('https://raw.githubusercontent.com/anhdanggit/atom-assignments/main/data/datacracy_groups.csv')\n",
    "#st.write(json_data['SLACK_BEARER_TOKEN'])\n",
    "\n",
    "@st.cache\n",
    "def load_users_df():\n",
    "    # Slack API User Data\n",
    "    endpoint = \"https://slack.com/api/users.list\"\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(json_data['SLACK_BEARER_TOKEN'])}\n",
    "    response_json = requests.post(endpoint, headers=headers).json() \n",
    "    user_dat = response_json['members']\n",
    "\n",
    "    # Convert to CSV\n",
    "    user_dict = {'user_id':[],'name':[],'display_name':[],'real_name':[],'title':[],'is_bot':[]}\n",
    "    for i in range(len(user_dat)):\n",
    "      user_dict['user_id'].append(user_dat[i]['id'])\n",
    "      user_dict['name'].append(user_dat[i]['name'])\n",
    "      user_dict['display_name'].append(user_dat[i]['profile']['display_name'])\n",
    "      user_dict['real_name'].append(user_dat[i]['profile']['real_name_normalized'])\n",
    "      user_dict['title'].append(user_dat[i]['profile']['title'])\n",
    "      user_dict['is_bot'].append(int(user_dat[i]['is_bot']))\n",
    "    user_df = pd.DataFrame(user_dict) \n",
    "    # Read dtc_group hosted in github\n",
    "    dtc_groups = pd.read_csv(DTC_GROUPS_URL)\n",
    "    user_df = user_df.merge(dtc_groups, how='left', on='name')\n",
    "    return user_df\n",
    "\n",
    "@st.cache\n",
    "def load_channel_df():\n",
    "    endpoint2 = \"https://slack.com/api/conversations.list\"\n",
    "    data = {'types': 'public_channel,private_channel'} # -> CHECK: API Docs https://api.slack.com/methods/conversations.list/test\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(SLACK_BEARER_TOKEN)}\n",
    "    response_json = requests.post(endpoint2, headers=headers, data=data).json() \n",
    "    channel_dat = response_json['channels']\n",
    "    channel_dict = {'channel_id':[], 'channel_name':[], 'is_channel':[],'creator':[],'created_at':[],'topics':[],'purpose':[],'num_members':[]}\n",
    "    for i in range(len(channel_dat)):\n",
    "        channel_dict['channel_id'].append(channel_dat[i]['id'])\n",
    "        channel_dict['channel_name'].append(channel_dat[i]['name'])\n",
    "        channel_dict['is_channel'].append(channel_dat[i]['is_channel'])\n",
    "        channel_dict['creator'].append(channel_dat[i]['creator'])\n",
    "        channel_dict['created_at'].append(dt.fromtimestamp(float(channel_dat[i]['created'])))\n",
    "        channel_dict['topics'].append(channel_dat[i]['topic']['value'])\n",
    "        channel_dict['purpose'].append(channel_dat[i]['purpose']['value'])\n",
    "        channel_dict['num_members'].append(channel_dat[i]['num_members'])\n",
    "    channel_df = pd.DataFrame(channel_dict) \n",
    "    return channel_df\n",
    "\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def load_msg_dict():\n",
    "    endpoint3 = \"https://slack.com/api/conversations.history\"\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(SLACK_BEARER_TOKEN)}\n",
    "    msg_dict = {'channel_id':[],'msg_id':[], 'msg_ts':[], 'user_id':[], 'latest_reply':[],'reply_user_count':[],'reply_users':[],'github_link':[],'text':[]}\n",
    "    for channel_id, channel_name in zip(channel_df['channel_id'], channel_df['channel_name']):\n",
    "        print('Channel ID: {} - Channel Name: {}'.format(channel_id, channel_name))\n",
    "        try:\n",
    "            data = {\"channel\": channel_id} \n",
    "            response_json = requests.post(endpoint3, data=data, headers=headers).json()\n",
    "            msg_ls = response_json['messages']\n",
    "            for i in range(len(msg_ls)):\n",
    "                if 'client_msg_id' in msg_ls[i].keys():\n",
    "                    msg_dict['channel_id'].append(channel_id)\n",
    "                    msg_dict['msg_id'].append(msg_ls[i]['client_msg_id'])\n",
    "                    msg_dict['msg_ts'].append(dt.fromtimestamp(float(msg_ls[i]['ts'])))\n",
    "                    msg_dict['latest_reply'].append(dt.fromtimestamp(float(msg_ls[i]['latest_reply'] if 'latest_reply' in msg_ls[i].keys() else 0))) ## -> No reply: 1970-01-01\n",
    "                    msg_dict['user_id'].append(msg_ls[i]['user'])\n",
    "                    msg_dict['reply_user_count'].append(msg_ls[i]['reply_users_count'] if 'reply_users_count' in msg_ls[i].keys() else 0)\n",
    "                    msg_dict['reply_users'].append(msg_ls[i]['reply_users'] if 'reply_users' in msg_ls[i].keys() else 0) \n",
    "                    msg_dict['text'].append(msg_ls[i]['text'] if 'text' in msg_ls[i].keys() else 0) \n",
    "                    ## -> Censor message contains tokens\n",
    "                    text = msg_ls[i]['text']\n",
    "                    github_link = re.findall('(?:https?://)?(?:www[.])?github[.]com/[\\w-]+/?', text)\n",
    "                    msg_dict['github_link'].append(github_link[0] if len(github_link) > 0 else None)\n",
    "        except:\n",
    "            print('====> '+ str(response_json))\n",
    "    msg_df = pd.DataFrame(msg_dict)\n",
    "    return msg_df\n",
    "\n",
    "def process_msg_data(msg_df, user_df, channel_df):\n",
    "    ## Extract 2 reply_users\n",
    "    msg_df['reply_user1'] = msg_df['reply_users'].apply(lambda x: x[0] if x != 0 else '')\n",
    "    msg_df['reply_user2'] = msg_df['reply_users'].apply(lambda x: x[1] if x != 0 and len(x) > 1 else '')\n",
    "    ## Merge to have a nice name displayed\n",
    "    msg_df = msg_df.merge(user_df[['user_id','name','DataCracy_role']].rename(columns={'name':'submit_name'}), \\\n",
    "        how='left',on='user_id')\n",
    "    msg_df = msg_df.merge(user_df[['user_id','name']].rename(columns={'name':'reply1_name','user_id':'reply1_id'}), \\\n",
    "        how='left', left_on='reply_user1', right_on='reply1_id')\n",
    "    msg_df = msg_df.merge(user_df[['user_id','name']].rename(columns={'name':'reply2_name','user_id':'reply2_id'}), \\\n",
    "        how='left', left_on='reply_user2', right_on='reply2_id')\n",
    "    ## Merge for nice channel name\n",
    "    msg_df = msg_df.merge(channel_df[['channel_id','channel_name','created_at']], how='left',on='channel_id')\n",
    "    ## Format datetime cols\n",
    "    msg_df['created_at'] = msg_df['created_at'].dt.strftime('%Y-%m-%d')\n",
    "    msg_df['msg_date'] = msg_df['msg_ts'].dt.strftime('%Y-%m-%d')\n",
    "    msg_df['msg_time'] = msg_df['msg_ts'].dt.strftime('%H:%M')\n",
    "    msg_df['wordcount'] = msg_df.text.apply(lambda s: len(s.split()))\n",
    "    return msg_df\n",
    "\n",
    "\n",
    "# Table data\n",
    "user_df = load_users_df()\n",
    "print('user')\n",
    "channel_df = load_channel_df()\n",
    "msg_df = load_msg_dict()\n",
    "\n",
    "#st.write(process_msg_data(msg_df, user_df, channel_df))\n",
    "\n",
    "\n",
    "# Input\n",
    "st.sidebar.markdown('## Thông tin')\n",
    "user_id = st.sidebar.text_input(\"Nhập Mã Số Người Dùng\", 'U01xxxx')\n",
    "\n",
    "\n",
    "valid_user_id = user_df['user_id'].str.contains(user_id).any()\n",
    "if valid_user_id:\n",
    "    filter_user_df = user_df[user_df.user_id == user_id] ## dis = display =]]\n",
    "    filter_msg_df = msg_df[(msg_df.user_id == user_id) | (msg_df.reply_user1 == user_id) | (msg_df.reply_user2 == user_id)]\n",
    "    p_msg_df = process_msg_data(filter_msg_df, user_df, channel_df)\n",
    "\n",
    "    ## Submission\n",
    "    submit_df = p_msg_df[p_msg_df.channel_name.str.contains('assignment')]\n",
    "    submit_df = submit_df[submit_df.DataCracy_role.str.contains('Learner')]\n",
    "    submit_df = submit_df[submit_df.user_id == user_id]\n",
    "    latest_ts = submit_df.groupby(['channel_name', 'user_id']).msg_ts.idxmax() ## -> Latest ts\n",
    "    submit_df = submit_df.loc[latest_ts]\n",
    "    dis_cols1 = ['channel_name', 'created_at','msg_date','msg_time','reply_user_count', 'reply1_name']\n",
    "    \n",
    "    # Review\n",
    "    review_df = p_msg_df[p_msg_df.user_id != user_id] ##-> Remove the case self-reply\n",
    "    review_df = review_df[review_df.channel_name.str.contains('assignment')]\n",
    "    review_df = review_df[review_df.DataCracy_role.str.contains('Learner')]\n",
    "    dis_cols2 = ['channel_name', 'created_at','msg_date','msg_time','reply_user_count','submit_name']\n",
    "    \n",
    "    ## Discussion\n",
    "    discuss_df = p_msg_df[p_msg_df.channel_name.str.contains('discuss')]\n",
    "    discuss_df = discuss_df.sort_values(['msg_date','msg_time'])\n",
    "    dis_cols3 = ['channel_name','msg_date', 'msg_time','wordcount','reply_user_count','reply1_name']\n",
    "    \n",
    "    st.markdown('Hello **{}**!'.format(list(filter_user_df['real_name'])[0]))\n",
    "    st.write(filter_user_df)\n",
    "    st.markdown('## Lịch sử Nộp Assignment')\n",
    "    st.write(submit_df[dis_cols1])\n",
    "    st.markdown('## Lịch sử Review Assignment')\n",
    "    st.write(review_df[dis_cols2])\n",
    "    st.markdown('## Lịch sử Discussion')\n",
    "    st.write(discuss_df[dis_cols3])\n",
    "\n",
    "    # Number cards on Sidebar\n",
    "    st.sidebar.markdown(f'''<div class=\"card text-info bg-info mb-3\" style=\"width: 18rem\">\n",
    "    <div class=\"card-body\">\n",
    "    <h5 class=\"card-title\">ĐÃ NỘP</h5>\n",
    "    <p class=\"card-text\">{len(submit_df):02d}</p>\n",
    "    </div>\n",
    "    </div>''', unsafe_allow_html=True)\n",
    "\n",
    "    review_cnt = 100 * len(submit_df[submit_df.reply_user_count > 0])/len(submit_df) if len(submit_df) > 0  else 0\n",
    "    st.sidebar.markdown(f'''<div class=\"card text-info bg-info mb-3\" style=\"width: 18rem\">\n",
    "    <div class=\"card-body\">\n",
    "    <h5 class=\"card-title\">ĐƯỢC REVIEW</h5>\n",
    "    <p class=\"card-text\">{review_cnt:.0f}%</p>\n",
    "    </div>\n",
    "    </div>''', unsafe_allow_html=True)\n",
    "\n",
    "    st.sidebar.markdown(f'''<div class=\"card text-info bg-info mb-3\" style=\"width: 18rem\">\n",
    "    <div class=\"card-body\">\n",
    "    <h5 class=\"card-title\">ĐÃ REVIEW</h5>\n",
    "    <p class=\"card-text\">{len(review_df):02d}</p>\n",
    "    </div>\n",
    "    </div>''', unsafe_allow_html=True)\n",
    "\n",
    "    st.sidebar.markdown(f'''<div class=\"card text-info bg-info mb-3\" style=\"width: 18rem\">\n",
    "    <div class=\"card-body\">\n",
    "    <h5 class=\"card-title\">THẢO LUẬN</h5>\n",
    "    <p class=\"card-text\">{sum(discuss_df['wordcount']):,d} chữ</p>\n",
    "    </div>\n",
    "    </div>''', unsafe_allow_html=True)\n",
    "    \n",
    "else:\n",
    "    st.markdown('Không tìm thấy Mã Số {}'.format(user_id))\n",
    "\n",
    "## Run: streamlit run streamlit/datacracy_slack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\r\n",
      "\r\n",
      "Error: Invalid value: File does not exist: datacracy_slack.py\r\n"
     ]
    }
   ],
   "source": [
    "!streamlit run datacracy_slack.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "home_assignment_5.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
